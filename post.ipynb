{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f1b656",
   "metadata": {},
   "source": [
    "TODO NEW PLAN:\n",
    "- First example just do toy / fake data - sample from gaussian, learn parameters of gaussian, and spline version of density\n",
    "- Second example do GD-1 sky positions - coarse filter CMD and proper motions and fit 2D sky positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bce18a",
   "metadata": {},
   "source": [
    "A problem I have run into frequently in astronomical data analysis is the need to infer parameters of a density model where some aspects of the model are allowed to be flexible and other components are held more rigid. In these contexts, we are also sometimes interested in learning a flexible representation for the density of sources itself. This post demonstrates how to implement models with flexibility controlled by spline interpolation of function values in [JAX](https://jax.readthedocs.io/en/latest/).\n",
    "\n",
    "One example of the need for flexibility in density modeling is the classic Galactic astronomy problem of measuring the vertical stellar density profile (and midplane density) of the Galactic disk: In this problem, we start with observations of stellar positions ($x, y, z$) (probably observed under some selection function) and we want to infer the midplane density value and a model for the density profile away from the midplane. Historically, simple, parametric density profiles have been used (e.g., [Bovy et al. 2017](https://ui.adsabs.harvard.edu/abs/2017MNRAS.470.1360B/abstract)), but we now know that there are significant asymmetries in the density of stars (e.g., [Bennett et al. 2019](https://ui.adsabs.harvard.edu/abs/2019MNRAS.482.1417B/abstract)), and so we might now want to fit a parametric density profile plus a model component to handle this asymmetry.\n",
    "\n",
    "Another problem where the need to fit models with parametric and flexible components arises is in modeling the phase-space density of stellar streams (e.g., [Koposov et al. 2019](https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.4726K/abstract), [Tavangar et al. 2022](https://ui.adsabs.harvard.edu/abs/2022ApJ...925..118T/abstract)). In the case of stellar streams, we generally want to simultaneously fit the \"track\" or ridgeline of the stream in position and velocity components, the width of the stream, the density along the stream, and a flexible model for the background stellar density in these components.\n",
    "\n",
    "There are many possible options for adding flexibility to models (see: Machine Learning). One particularly useful tool that is used heavily in time series analysis are [Gaussian processes](https://en.wikipedia.org/wiki/Gaussian_process) (GPs). GPs allow adding controlled flexibility in probabilistic models (i.e. weakly parametric, through specification of a kernel function) and have gained popularity in astronomy recently thanks to advances in computational efficiency in computing GP likelihoods (e.g., [celerite](https://github.com/exoplanet-dev/celerite2) or [tinygp](https://github.com/dfm/tinygp)). I won't go over GPs in this post, but there are many resources available online and on GitHub that give great introductions to GPs (e.g., [Dan Foreman-Mackey's slides](https://speakerdeck.com/dfm/an-astronomers-introduction-to-gaussian-processes-v2) or [Rodrigo Luger's tutorial](https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions/blob/main/Sessions/Session13/Day2/answers/01-Introduction-to-GPs.ipynb)).\n",
    "\n",
    "In this post, we will use another frequently-used tool for specifying flexible models: [cubic splines](https://en.wikipedia.org/wiki/Spline_(mathematics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global imports we will need throughout this post:\n",
    "import astropy.units as u\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc251c",
   "metadata": {},
   "source": [
    "# Spline models\n",
    "\n",
    "A spline function is fully determined by the degree of the polynomial used, the location of $M$ \"knots\" $x_m$, and the function value at the knots $f_m$. A common choice for the polynomial degree is 3, or cubic splines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "M = 8\n",
    "x_m = np.linspace(0, 10, M)\n",
    "f_m = rng.uniform(-1, 1, M)\n",
    "spl = sci.InterpolatedUnivariateSpline(x_m, f_m, k=3)  # k = the polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f35a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_m, f_m)\n",
    "\n",
    "grid = np.linspace(-1, 11, 1024)\n",
    "plt.plot(grid, spl(grid), marker=\"\", linestyle=\"-\", color=\"tab:blue\", zorder=-10)\n",
    "\n",
    "plt.annotate(\n",
    "    \"knots\",\n",
    "    xy=(x_m[0], f_m[0]),\n",
    "    xytext=(2, 2),\n",
    "    arrowprops=dict(color=\"#666\", shrinkB=4, arrowstyle=\"->\"),\n",
    "    ha=\"center\",\n",
    ")\n",
    "for m in range(1, 3):\n",
    "    plt.annotate(\n",
    "        \"     \",\n",
    "        xy=(x_m[m], f_m[m]),\n",
    "        xytext=(2, 2),\n",
    "        arrowprops=dict(color=\"#666\", shrinkB=4, arrowstyle=\"->\"),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf2bd7",
   "metadata": {},
   "source": [
    "The task of finding a spline representation of a function given samples or points is sometimes called \"spline regression.\" The problem is straightforward if we pick and fix locations for the knots of the spline function we want to fit and then add into our model the values of the function at the locations of the knots. This type of model has the advantage that the (spatial) scale of flexibility or \"degrees of freedom\" is controllable by setting the number of knots. However, unlike in GPs where kernel functions can be used to parametrize the amplitude or spatial scales of your problem, these things are not explicitly controlled in a spline model. One other disadvantage of a spline model is that the number of parameters in your model grows as you increase the number of knots (i.e. the degrees of freedom) of the model -- this can make spline models intractable in some simple optimization routines (e.g., using `scipy.minimize` without gradient information) or in some Markov Chain Monte Carlo (MCMC) methods that do not use gradient information (e.g., Metropolis-Hastings or [`emcee`](https://emcee.readthedocs.io/en/stable/)).\n",
    "\n",
    "Fortunately, it is possible to use spline models with [JAX](https://jax.readthedocs.io/), which automatically gives us access to functional gradients and therefore opens up the possibility of using optimization and sampling methods that perform well with large numbers of parameters. Below are two examples that demonstrate how to implement spline components in density models using JAX, to optimize the parameters of the models with [`jaxopt`](https://jaxopt.github.io/), and to generate posterior samples using Hamiltonian Monte Carlo with [`blackjax`](https://blackjax-devs.github.io/blackjax/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a518b4",
   "metadata": {},
   "source": [
    "# Example: Fitting a 1D density profile with splines\n",
    "\n",
    "As a first demonstration of the idea, we are going to use simulated data to mock up a simpler version of the vertical density problem mentioned above. We will generate simulated data from a Gaussian, and then show how to fit the density distribution by modeling the points as an [inhomogeneous Poisson process](https://en.wikipedia.org/wiki/Poisson_point_process#Inhomogeneous_Poisson_point_process) with either (1) a Gaussian or (2) a cubic spline density function. In either case, given a density function $n(z)$ (Gaussian or spline), our likelihood and log-likelihood are given by the Poisson process likelihood, given all $N$ of our $z_n$ data points:\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\left\\{z_n\\right\\}_N \\,|\\, n(z)) &=\n",
    "    \\exp{\\left[-\\int {\\rm d}z \\, n(z)\\right]} \\, \\prod_n^N n(z_n)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Case 1: Gaussian model\n",
    "\n",
    "For our first demo, we will use a Gaussian to fit the data (which were generated by a Gaussian, so this is truly a toy example). In this case:\n",
    "$$\n",
    "\\begin{align}\n",
    "n(z \\,|\\, N_0, \\mu, \\sigma) &= N_0 \\, \\mathcal{N}(z \\,|\\, \\mu, \\sigma)\\\\\n",
    "\\mathcal{N}(x \\,|\\, \\mu, \\sigma) &= \\frac{1}{\\sqrt{2\\pi\\,\\sigma^2}} \\, e^{-\\,\\frac{(x - \\mu)^2}{2\\,\\sigma^2}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathcal{N}$ represents the normal distribution, $N_0$ is the total number of sources, and the mean $\\mu$ and standard deviation $\\sigma$ are the usual Gaussian parameters. \n",
    "\n",
    "The integral that appears in the first term of the Poisson process likelihood above is therefore just the total number $N_0$, as the integral over the normal distribution $\\mathcal{N}$ is 1:\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\left\\{z_n\\right\\}_N \\,|\\, N_0, \\mu, \\sigma) &= \\exp{\\left[-N_0 \\, \\int {\\rm d}z \\, \\mathcal{N}(z)\\right]}  \\, \\prod_n^N n(z_n)\\\\\n",
    "    &= e^{-n_0}  \\, N_0^N \\, \\prod_n^N \\mathcal{N}(z_n \\,|\\, \\mu, \\sigma)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The log-likelihood is therefore (where $N$ is the number of data points, and $N_0$ is a parameter):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln p(\\left\\{z_n\\right\\}_N \\,|\\, n_0, \\mu, \\sigma) &=\n",
    "    -N_0 + N\\,\\ln N_0 + \\sum_n^N \\ln \\mathcal{N}(z_n \\,|\\, \\mu, \\sigma)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8e7bb",
   "metadata": {},
   "source": [
    "To start with, we will generate some random, normal distributed points with arbitrarily chosen mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d807d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "N = 100_000\n",
    "z = rng.normal(0.03, 0.31, size=N)\n",
    "\n",
    "# Pack the data into a dictionary so later we can store other metadata. For\n",
    "# reasons that will be clear later, we also store the number of data points\n",
    "# in this dictionary data structure:\n",
    "data = {\"N\": N, \"z\": z}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d21c2",
   "metadata": {},
   "source": [
    "Let's start by making a histogram of the \"data\" to visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7416fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bins = np.linspace(-2, 2, 128)\n",
    "plt.hist(data[\"z\"], bins=z_bins)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"number of sources\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c4809e",
   "metadata": {},
   "source": [
    "To visualize an estimate of the density function, we can use the `numpy.histogram` function instead to compute the number counts per bin and divide by the size of each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, xe = np.histogram(data[\"z\"], bins=z_bins)\n",
    "xc = 0.5 * (xe[:-1] + xe[1:])\n",
    "dens = H / (xe[1] - xe[0])\n",
    "\n",
    "plt.plot(xc, dens, drawstyle=\"steps-mid\", marker=\"\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"density $n(z)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3adf2e1",
   "metadata": {},
   "source": [
    "In what follows, we are going to be defining several different density models and objective functions for our different density models. But ultimately, with all of these choices (e.g., Gaussian density model vs. cubic spline), we will need to be able to compute the log-likelihood given a choice of parameters. I like to use object-oriented programming (OOP) to structure my code when I am in situations like this because it helps to reduce duplicated code, enables encapsulation and namespacing, and, frankly, because I think the benefits of Python shine when using OOP. However, JAX is really designed to be used within a [*functional programming*](https://en.wikipedia.org/wiki/Functional_programming) context because of the way [Just-in-time](https://en.wikipedia.org/wiki/Just-in-time_compilation) (JIT) compilation works. You can read a bit more about this in the [JAX Gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) page, but the bottom line is that all JIT-compiled functions must be *pure functions* (functions that return the same values given the same input arguments). \n",
    "\n",
    "There are some advanced ways of implementing more OOP-like code with JAX, but here I'm going to (ab)use Python classes as a simple way of creating namespaces for the functions we will need with a light form of inheritance that still obeys the *pure function* requirement of JAX. These classes don't look like true OOP because we use `@classmethod`'s instead of regular instance methods, but some other OOP ideas still translate. We will start by defining a base `Model` class that implements some common methods we will need for any of the density models we implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to wrap JAX's jit function with a partial function call to get\n",
    "# it to work with our classmethod's below. We will use it to tell JAX to treat\n",
    "# the 0'th input (i.e. the class itself in a classmethod) as a compile-time\n",
    "# constant-valued object:\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class Model:\n",
    "    # This will store the parameter names and expected sizes of the parameters\n",
    "    # (to allow for array-valued parameters) for the density models we\n",
    "    # implement later on:\n",
    "    param_names = {}\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def unpack_pars(cls, p_arr):\n",
    "        \"\"\"\n",
    "        This function takes a parameter array and unpacks it into a dictionary\n",
    "        with the parameter names as keys.\n",
    "        \"\"\"\n",
    "        p_dict = {}\n",
    "        j = 0\n",
    "        for name, size in cls.param_names.items():\n",
    "            p_dict[name] = jnp.squeeze(p_arr[j : j + size])\n",
    "            j += size\n",
    "        return p_dict\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def pack_pars(cls, p_dict):\n",
    "        \"\"\"\n",
    "        This function takes a parameter dictionary and packs it into a JAX array\n",
    "        where the order is set by the parameter name list defined on the class.\n",
    "        \"\"\"\n",
    "        p_arrs = []\n",
    "        for name in cls.param_names.keys():\n",
    "            p_arrs.append(jnp.atleast_1d(p_dict[name]))\n",
    "        return jnp.concatenate(p_arrs)\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def ln_posterior(cls, pars_arr, data, *args):\n",
    "        pars = cls.unpack_pars(pars_arr)\n",
    "        return cls.ln_likelihood(pars, data, *args) + cls.ln_prior(pars)\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def objective(cls, pars_arr, N, data, *args):\n",
    "        \"\"\"\n",
    "        This function computes an objective function to be *minimized*: In our\n",
    "        case, we will be doing Bayesian statistics, so this is generally the\n",
    "        negative log-posterior-probability value such that if we minimize the\n",
    "        objective function, we obtain the maximum a posteriori (MAP) parameter\n",
    "        values. Here we also normalize the value by the number of data points so\n",
    "        that scipy's minimizers don't run into overflow issues with the\n",
    "        gradients.\n",
    "        \"\"\"\n",
    "        return -cls.ln_posterior(pars_arr, data, *args) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba71727",
   "metadata": {},
   "source": [
    "With our base `Model` class defined, we can now implement a subclass for the first model we are going to fit to our simulated data: a Gaussian! Using the true density model to fit the simulated data we should recover the input parameters that we used to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_normal(x, mu, var):\n",
    "    \"\"\"Evaluate the log-normal probability\"\"\"\n",
    "    return -0.5 * (jnp.log(2 * np.pi * var) + (x - mu) ** 2 / var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianModel(Model):\n",
    "    param_names = {\n",
    "        \"ln_N0\": 1,  # the log number density\n",
    "        \"mean\": 1,  # the mean of the Gaussian\n",
    "        \"ln_std\": 1,  # the log standard deviation\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def ln_density(x, ln_N0, mean, ln_std):\n",
    "        \"\"\"\n",
    "        This function implements the log-density of our model. Here, this is the\n",
    "        log-Gaussian.\n",
    "        \"\"\"\n",
    "        var = jnp.exp(2 * ln_std)\n",
    "        return ln_N0 + ln_normal(x, mean, jnp.exp(2 * ln_std))\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def ln_likelihood(cls, pars, data):\n",
    "        \"\"\"\n",
    "        Implementation of the log-likelihood for an inhomogeneous Poisson\n",
    "        process with underlying density (rate) function given by a Gaussian.\n",
    "        Here the integral over our density function is has a simple closed form\n",
    "        solution (see the math above).\n",
    "        \"\"\"\n",
    "        dens = cls.ln_density(data[\"z\"], **pars)\n",
    "        return -jnp.exp(pars[\"ln_N0\"]) + dens.sum()\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def ln_prior(cls, pars):\n",
    "        \"\"\"\n",
    "        A very light prior on the parameters. We again use Normal's for priors,\n",
    "        but for most parameters we use relatively wide (large variance) values\n",
    "        so that the prior does not have much of an influence.\n",
    "        \"\"\"\n",
    "        lp = 0.0\n",
    "\n",
    "        # A very wide, basically unconstrained Gaussian\n",
    "        lp += ln_normal(pars[\"ln_N0\"], 0, 100)\n",
    "\n",
    "        # We expect the mean to be close to 0\n",
    "        lp += ln_normal(pars[\"mean\"], 0, 1)\n",
    "\n",
    "        # We expect the standard deviation to be small:\n",
    "        lp += ln_normal(pars[\"ln_std\"], -2, 3)\n",
    "\n",
    "        return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba55fbd",
   "metadata": {},
   "source": [
    "Let's pick some initial values for our parameters and plot the density function corresponding to our parameter choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xc, dens, drawstyle=\"steps-mid\", marker=\"\")\n",
    "\n",
    "init_pars = {\"ln_N0\": np.log(N) + 0.5, \"mean\": 1e-1, \"ln_std\": np.log(0.3)}\n",
    "init_p = GaussianModel.pack_pars(init_pars)\n",
    "\n",
    "z_grid = np.linspace(z_bins.min(), z_bins.max(), 1024)\n",
    "plt.plot(z_grid, np.exp(GaussianModel.ln_density(z_grid, **init_pars)), marker=\"\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"density $n(z)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322699e",
   "metadata": {},
   "source": [
    "Those initial parameter values don't look like a very good match to the observed density, but it's probably close enough that an optimizer will be able to find a better solution from there. For the optimizer, we will use Scipy's [L-BFGS-B](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html) implementation, which is available through the general-purpose `scipy.optimize.minimize()` function. Here we use JAX's `value_and_grad()` to get a function handle based on our objective function that returns both the objective value and the gradient with respect to the input parameters. This is where the utility of JAX comes to light: it uses auto-differentiation to compute the gradients for us. We have to set `jac=True` in `minimize()` to tell Scipy to expect the gradient along with the objective function value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4129fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c157d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sco.minimize(\n",
    "    jax.value_and_grad(GaussianModel.objective),\n",
    "    GaussianModel.pack_pars(init_pars),\n",
    "    args=(len(data[\"z\"]), data),\n",
    "    jac=True,\n",
    "    method=\"l-bfgs-b\",\n",
    "    options=dict(maxiter=1000),\n",
    "    bounds=[(5, 20), (-2, 2), (-5, 5)],\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300442f",
   "metadata": {},
   "source": [
    "It looks like that optimization completed successfully, and after only 10 function evaluations! Let's look at the density function implied by the optimized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2030813",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_pars = GaussianModel.unpack_pars(res.x)\n",
    "\n",
    "plt.plot(xc, dens, drawstyle=\"steps-mid\", marker=\"\")\n",
    "\n",
    "z_grid = np.linspace(z_bins.min(), z_bins.max(), 1024)\n",
    "plt.plot(\n",
    "    z_grid,\n",
    "    np.exp(GaussianModel.ln_density(z_grid, **opt_pars)),\n",
    "    marker=\"\",\n",
    "    color=\"tab:green\",\n",
    ")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"density $n(z)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47126f",
   "metadata": {},
   "source": [
    "That looks like a pretty good fit! Let's move on to a more flexible example.\n",
    "\n",
    "## Case 2: Spline model\n",
    "\n",
    "We will now replace our density model $n(z)$ with a cubic spline representation of the function. We will fix the location of the spline knots by using a hard-set, uniform grid of points in $z$, but the parameters of the model will then be the value of the (log-)density at the locations of the knots. Though there is no jax-ified cubic interpolation built-in to JAX itself (as far as I can tell, it currently only supports linear interpolation), we will use another package — `jax_cosmo` — which provides a jax-aware version of Scipy's `InterpolatedUnivariateSpline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99508dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_cosmo.scipy.interpolate import InterpolatedUnivariateSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e24fe1",
   "metadata": {},
   "source": [
    "Awesome, we now have the main tool we need to enable implementing the spline model, however we have one more mathematical / numerical hurdle to solve: we need to be able to compute the integral of our density model to compute the first term in the Poisson process likelihood\n",
    "$$\n",
    "\\exp{\\left[-\\int {\\rm d}z \\, n(z)\\right]}\n",
    "$$\n",
    "\n",
    "For generic cubic splines, this integral over all $z$'s is not finite. We therefore have to pick a domain over which to do this integral, and this then slightly changes the meaning of our parameter `ln_N0` to be the number of sources *in the domain we choose*. In practice, if we pick a domain that is large enough and the density function falls off quickly (as it does here), there won't be any practical difference. (But note: if you have a rigid selection region, or if you pick a domain that truncates the data, you have to be more careful than me!) Since our data end around $z\\sim \\pm 1.5$, we will pick a window of $(-3, 3)$. \n",
    "\n",
    "We now need a way of computing the integral of our spline model over this domain. If our parameters were the value of the *density* $N_0$ at the locations of the knots, we could use the `InterpolatedUnivariateSpline.integral()` method directly to compute the integral. However, we use the value of the log-density as parameters, so the integral is not as straightforward. Here, I've implemented a version of [Simpson's rule](https://en.wikipedia.org/wiki/Simpson%27s_rule) that takes in the log-function values and returns the log-integral, which is more stable than using other integration tools that would require first exponentiating the density and then taking the log of the estimated integral value on the outside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_simpson(ln_y, x):\n",
    "    \"\"\"\n",
    "    Evaluate the log of the definite integral of a function evaluated on a\n",
    "    grid using Simpson's rule\n",
    "    \"\"\"\n",
    "\n",
    "    dx = jnp.diff(x)[0]\n",
    "    num_points = len(x)\n",
    "    if num_points // 2 == num_points / 2:\n",
    "        raise ValueError(\"Because of laziness, the input size must be odd\")\n",
    "\n",
    "    weights_first = jnp.asarray([1.0])\n",
    "    weights_mid = jnp.tile(jnp.asarray([4.0, 2.0]), [(num_points - 3) // 2])\n",
    "    weights_last = jnp.asarray([4.0, 1.0])\n",
    "    weights = jnp.concatenate([weights_first, weights_mid, weights_last], axis=0)\n",
    "\n",
    "    return jax.scipy.special.logsumexp(ln_y + jnp.log(weights), axis=-1) + jnp.log(\n",
    "        dx / 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5dc727",
   "metadata": {},
   "source": [
    "With a decision about our integration window and a jax-ified function to compute the value of the log-integral over our spline density function, we can now set up a spline model to fit our toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94459c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSplineModel(Model):\n",
    "    knots = jnp.linspace(-3, 3, 11)  # locations of the spline knots\n",
    "    param_names = {\n",
    "        \"ln_n0\": 11,  # the value of the log-density at the knots\n",
    "    }\n",
    "    window = (-3, 3)  # integration window for numerical integral of density\n",
    "    n_integral_pts = 1025  # the number of integration grid points to use\n",
    "\n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def ln_density(x, ln_n0, knots):\n",
    "        \"\"\"\n",
    "        The log-density is just an evaluation of the spline at the input\n",
    "        \"\"\"\n",
    "        ln_dens_spl = InterpolatedUnivariateSpline(knots, ln_n0, k=3)\n",
    "        return ln_dens_spl(x)\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def ln_likelihood(cls, pars, data):\n",
    "        \"\"\"\n",
    "        Implementation of the log-likelihood for an inhomogeneous Poisson\n",
    "        process with underlying density (rate) function given by a spline\n",
    "        \"\"\"\n",
    "        ln_dens = cls.ln_density(data[\"z\"], pars[\"ln_n0\"], cls.knots)\n",
    "\n",
    "        # As mentioned above, to compute the integral over the density, we do\n",
    "        # the integral numerically using Simpson's rule. For my implementation,\n",
    "        # we must pass in a grid of points and the log of the function to\n",
    "        # integrate evaluated at these grid points. The number of grid points is\n",
    "        # hard-set here, but this should be tuned to meet some accuracy criteria\n",
    "        V_grid = jnp.linspace(*cls.window, cls.n_integral_pts)\n",
    "        ln_V = ln_simpson(cls.ln_density(V_grid, pars[\"ln_n0\"], cls.knots), V_grid)\n",
    "        return -jnp.exp(ln_V) + ln_dens.sum()\n",
    "\n",
    "    @classmethod\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def ln_prior(cls, pars):\n",
    "        lp = 0.0\n",
    "        for name, p in pars.items():\n",
    "            lp += ln_normal(p, 0, 100).sum()\n",
    "        return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c482c4",
   "metadata": {},
   "source": [
    "With our spline model defined, our integration window set, and our knot locations fixed, we now need to initialize our parameters: the log-density values at the knot locations. We do this by interpolating the estimated density we got from the histogram above at the location of the knots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "knots = np.linspace(xc.min(), xc.max(), 11)\n",
    "knots_ln_dens = sci.InterpolatedUnivariateSpline(xc, np.log(dens + 1e-8), k=3)(\n",
    "    GaussianSplineModel.knots\n",
    ")\n",
    "\n",
    "plt.plot(xc, dens, drawstyle=\"steps-mid\", marker=\"\")\n",
    "plt.scatter(\n",
    "    GaussianSplineModel.knots,\n",
    "    np.exp(knots_ln_dens),\n",
    "    color=\"tab:blue\",\n",
    "    label=\"knots - initial\",\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"density $n(z)$\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff748fe0",
   "metadata": {},
   "source": [
    "We can now run the optimizer with our new spline model for the density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addde9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add a little bit of random scatter just to make things interesting...\n",
    "init_pars = {\"ln_n0\": knots_ln_dens + rng.uniform(0, 0.1, size=len(knots))}\n",
    "res = sco.minimize(\n",
    "    jax.value_and_grad(GaussianSplineModel.objective),\n",
    "    GaussianSplineModel.pack_pars(init_pars),\n",
    "    args=(len(data[\"z\"]), data),\n",
    "    jac=True,\n",
    "    method=\"l-bfgs-b\",\n",
    "    options=dict(maxiter=1000, maxls=1000),  # I had to increase maxls to succeed\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a743d",
   "metadata": {},
   "source": [
    "Let's plot our optimized spline density function and the value at the knots over our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xc, dens, drawstyle=\"steps-mid\", marker=\"\")\n",
    "\n",
    "opt_pars = GaussianSplineModel.unpack_pars(res.x)\n",
    "\n",
    "_grid = np.linspace(*GaussianSplineModel.window, 1024)\n",
    "plt.plot(\n",
    "    _grid,\n",
    "    np.exp(\n",
    "        GaussianSplineModel.ln_density(\n",
    "            _grid, opt_pars[\"ln_n0\"], GaussianSplineModel.knots\n",
    "        )\n",
    "    ),\n",
    "    marker=\"\",\n",
    "    color=\"tab:green\",\n",
    ")\n",
    "plt.scatter(\n",
    "    GaussianSplineModel.knots,\n",
    "    np.exp(opt_pars[\"ln_n0\"]),\n",
    "    color=\"tab:blue\",\n",
    "    label=\"knots - optimized\",\n",
    ")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"density $n(z)$\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4aa80",
   "metadata": {},
   "source": [
    "Excellent - this also looks like a great fit! Unexpectedly, weird things happen outside of the range where we have data, but here the splines will largely be unconstrained.\n",
    "\n",
    "Now that we have optimized parameters, and we have a model class with a function that computes the log-posterior probability given data, we can use the optimized parameter values to initialize an MCMC sampling of the parameters. To do this, we will use the [blackjax](https://blackjax-devs.github.io/blackjax/) package to run a Hamiltonian Monte Carlo (HMC) sampler. In detail, we will use the NUTS sampler, and we will use their implementation of a window adaptation method to find good choices for the sampler parameters (step size and inverse mass matrix). This code follows the example code from the \"[quick introduction to blackjax](https://blackjax-devs.github.io/blackjax/examples/Introduction.html#nuts)\" in the blackjax documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main loop that does the sampling for us:\n",
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, _ = kernel(rng_key, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00375821",
   "metadata": {},
   "source": [
    "We start by using the blackjax implementation of STAN's window adaptation method to find good choices for the NUTS sampler parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabe20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)\n",
    "\n",
    "fn = jax.tree_util.Partial(GaussianSplineModel.ln_posterior, data=data)\n",
    "warmup = blackjax.window_adaptation(\n",
    "    blackjax.nuts,\n",
    "    fn,\n",
    "    1000,\n",
    ")\n",
    "\n",
    "state, kernel, _ = warmup.run(\n",
    "    rng_key,\n",
    "    res.x,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56d777",
   "metadata": {},
   "source": [
    "With a tuned kernel, we can now run our main inference loop to generate 1000 posterior samples of our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = inference_loop(rng_key, kernel, state, 1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691556d3",
   "metadata": {},
   "source": [
    "Let's see how these look! As a quick check, let's look at a trace plot of a few of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04779cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 5, 8]:\n",
    "    plt.plot(states.position[:, i], marker=\"\", drawstyle=\"steps-mid\")\n",
    "\n",
    "plt.xlabel(\"NUTS step\")\n",
    "plt.ylabel(\"parameter value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd5f6e",
   "metadata": {},
   "source": [
    "Those look stable! Of course, we should compute some convergence statistics and check on the quality of our samples. But for now, we'll assume that our \"by-eye\" test is good enough, and proceed :).\n",
    "\n",
    "For each posterior sample of our parameters, let's compute the implied density function and store these in a big 2D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_grid = np.linspace(*GaussianSplineModel.window, 1024)\n",
    "ln_dens_samples = np.zeros((states.position.shape[0], len(_grid)))\n",
    "for i, params in enumerate(states.position):\n",
    "    ln_dens_samples[i] = GaussianSplineModel.ln_density(\n",
    "        _grid, params, GaussianSplineModel.knots\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c27ac",
   "metadata": {},
   "source": [
    "We can now visualize the 16–84th percentile range of our inferred density function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.fill_between(\n",
    "    _grid,\n",
    "    *np.percentile(np.exp(ln_dens_samples), [16, 84], axis=0),\n",
    "    color=\"tab:blue\",\n",
    "    alpha=0.4,\n",
    "    linewidth=0\n",
    ")\n",
    "plt.plot(\n",
    "    _grid,\n",
    "    np.median(np.exp(ln_dens_samples), axis=0),\n",
    "    color=\"tab:blue\",\n",
    "    linewidth=2,\n",
    "    marker=\"\",\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"density $n(z)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bad2d",
   "metadata": {},
   "source": [
    "As we might expect, the density function is not well constrained beyond $|z|\\gtrsim 2$ or so where we do not have data.\n",
    "\n",
    "Now that we have a better picture of how to implement these types of flexible spline models with JAX, let's turn to a more complicated science example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90af31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example: Fitting the density profile and track of a stellar stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyia import GaiaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GaiaData(\"data/gd1-gaiadr3-blog.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(\n",
    "    g.gd1_phi1,\n",
    "    g.gd1_phi2,\n",
    "    marker=\"o\",\n",
    "    markeredgewidth=0,\n",
    "    markersize=3.0,\n",
    "    ls=\"none\",\n",
    "    alpha=0.25,\n",
    ")\n",
    "ax.set_xlim(-100, 20)\n",
    "ax.set_ylim(-10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd158e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db23f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a6bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419149f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076cc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5cfcbb2",
   "metadata": {},
   "source": [
    "Make some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c16aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "window = (-20, 20)\n",
    "rng = np.random.default_rng(42)\n",
    "# phi1_data = rng.uniform(0, 10, size=1024)\n",
    "phi1_data = rng.normal(5, 4, size=1024)\n",
    "assert np.all((phi1_data < window[1]) & (phi1_data > window[0]))\n",
    "\n",
    "phi2_data = rng.normal(1.5 * np.cos(2 * np.pi * phi1_data / 5), 0.8)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.scatter(phi1_data, phi2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def jln_normal(x, mu, var):\n",
    "    return -0.5 * (jnp.log(2 * np.pi * var) + (x - mu) ** 2 / var)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def phi2_ln_likelihood(\n",
    "    phi2_mean_knots,\n",
    "    phi2_mean_vals,\n",
    "    phi2_std_knots,\n",
    "    phi2_std_vals,\n",
    "    phi1_eval,\n",
    "    phi2_eval,\n",
    "):\n",
    "\n",
    "    phi2_mean = InterpolatedUnivariateSpline(phi2_mean_knots, phi2_mean_vals, k=3)\n",
    "\n",
    "    phi2_std = InterpolatedUnivariateSpline(phi2_std_knots, phi2_std_vals, k=3)\n",
    "\n",
    "    phi2_mean_model = phi2_mean(phi1_eval)\n",
    "    phi2_var_model = phi2_std(phi1_eval) ** 2\n",
    "\n",
    "    # phi2_mean_model = jnp.interp(phi1_eval, phi2_mean_knots, phi2_mean_vals)\n",
    "    # phi2_var_model = jnp.interp(phi1_eval, phi2_std_knots, phi2_std_vals) ** 2\n",
    "\n",
    "    return jln_normal(phi2_eval, phi2_mean_model, phi2_var_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a403d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2_knots = np.linspace(phi1_data.min(), phi1_data.max(), 25)\n",
    "\n",
    "dp2 = phi2_knots[1] - phi2_knots[0]\n",
    "_bins = np.linspace(\n",
    "    phi2_knots[0] - dp2 / 2, phi2_knots[-1] + dp2 / 2, len(phi2_knots) + 1\n",
    ")\n",
    "stat = binned_statistic(phi1_data, phi2_data, bins=_bins, statistic=np.nanmean)\n",
    "phi2_vals = stat.statistic\n",
    "phi2_vals[np.isnan(phi2_vals)] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "plt.scatter(phi1_data, phi2_data)\n",
    "plt.scatter(phi2_knots, phi2_vals, color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13763f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2_ln_likelihood(\n",
    "    phi2_knots,\n",
    "    phi2_vals,\n",
    "    phi2_knots,\n",
    "    np.full_like(phi2_knots, 0.4),\n",
    "    phi1_data,\n",
    "    phi2_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c983258",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(phi1_data.min(), phi1_data.max(), 256), np.linspace(-4, 4, 128)\n",
    ")\n",
    "zz = phi2_ln_likelihood(\n",
    "    phi2_knots, phi2_vals, phi2_knots, np.full_like(phi2_knots, 0.4), xx, yy\n",
    ")\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.pcolormesh(xx, yy, np.exp(zz), shading=\"auto\")\n",
    "plt.scatter(phi1_data, phi2_data, color=\"tab:blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def ln_prob(pars, data, phi2_knots):\n",
    "    n_phi2 = len(phi2_knots)\n",
    "    phi2_means = pars[:n_phi2]\n",
    "    ln_phi2_stds = pars[n_phi2 : 2 * n_phi2]\n",
    "    phi2_stds = jnp.exp(ln_phi2_stds)\n",
    "\n",
    "    ll = phi2_ln_likelihood(\n",
    "        phi2_knots, phi2_means, phi2_knots, phi2_stds, data[\"phi1\"], data[\"phi2\"]\n",
    "    ).sum()\n",
    "\n",
    "    lp = jln_normal(phi2_means, 0, 2.0).sum()\n",
    "    lp += jln_normal(ln_phi2_stds, -1, 1.0).sum()\n",
    "\n",
    "    return ll + lp\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def objective(pars, data, phi2_knots):\n",
    "    return -ln_prob(pars, data, phi2_knots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b83ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.concatenate((phi2_vals, np.log(np.full_like(phi2_knots, 0.4))))\n",
    "data = {\"phi1\": phi1_data, \"phi2\": phi2_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c056e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ScipyMinimize(method=\"l-bfgs-b\", fun=objective)\n",
    "res = solver.run(init_params, data=data, phi2_knots=phi2_knots)\n",
    "res.state.iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(phi2_knots, res.params[: len(phi2_knots)])\n",
    "plt.plot(phi2_knots, res.params[len(phi2_knots) :], color=\"tab:red\")\n",
    "plt.scatter(phi1_data, phi2_data, color=\"tab:blue\", s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(phi1_data.min(), phi1_data.max(), 256), np.linspace(-4, 4, 128)\n",
    ")\n",
    "zz = np.exp(\n",
    "    phi2_ln_likelihood(\n",
    "        phi2_knots,\n",
    "        res.params[: len(phi2_knots)],\n",
    "        phi2_knots,\n",
    "        np.exp(res.params[len(phi2_knots) :]),\n",
    "        xx,\n",
    "        yy,\n",
    "    )\n",
    ")\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.pcolormesh(\n",
    "    xx,\n",
    "    yy,\n",
    "    zz,\n",
    "    shading=\"auto\",\n",
    "    vmin=np.median(zz[(xx > 0) & (xx < 5)]),\n",
    "    vmax=np.max(zz[(xx > 0) & (xx < 5)]),\n",
    ")\n",
    "plt.scatter(phi1_data, phi2_data, color=\"tab:blue\", s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db3974",
   "metadata": {},
   "source": [
    "Copied from the `blackjax` getting started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, _ = kernel(rng_key, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = jax.tree_util.Partial(ln_prob, data=data, phi2_knots=phi2_knots)\n",
    "warmup = blackjax.window_adaptation(\n",
    "    blackjax.nuts,\n",
    "    fn,\n",
    "    1000,\n",
    ")\n",
    "\n",
    "state, kernel, _ = warmup.run(\n",
    "    rng_key,\n",
    "    res.params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = inference_loop(rng_key, kernel, state, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f246d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "for i in np.random.choice(states.position.shape[0], size=32):\n",
    "    params = states.position[i]\n",
    "    plt.plot(phi2_knots, params[: len(phi2_knots)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1103f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "for i in np.random.choice(states.position.shape[0], size=32):\n",
    "    params = states.position[i]\n",
    "    plt.plot(phi2_knots, params[len(phi2_knots) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd552a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Infer log-density as well:\n",
    "\n",
    "Inhomogeneous poisson point process: http://people.ee.duke.edu/~lcarin/PoissonProcess.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75802d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_simpson(ln_y, x, dtype=None):\n",
    "    \"\"\"Evaluates definite integral using Simpson's 1/3 rule\"\"\"\n",
    "\n",
    "    dx = jnp.diff(x)[0]\n",
    "    num_points = len(x)\n",
    "    if num_points // 2 == num_points / 2:\n",
    "        raise ValueError(\"oopsies\")\n",
    "\n",
    "    weights_first = jnp.asarray([1.0], dtype=dtype)\n",
    "    weights_mid = jnp.tile(\n",
    "        jnp.asarray([4.0, 2.0], dtype=dtype), [(num_points - 3) // 2]\n",
    "    )\n",
    "    weights_last = jnp.asarray([4.0, 1.0], dtype=dtype)\n",
    "    weights = jnp.concatenate([weights_first, weights_mid, weights_last], axis=0)\n",
    "\n",
    "    return jax.scipy.special.logsumexp(ln_y + jnp.log(weights), axis=-1) + jnp.log(\n",
    "        dx / 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10795c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def phi1_ln_likelihood(phi1_knots, ln_phi1_rate, phi1_eval):\n",
    "    ln_phi1_rate_spl = InterpolatedUnivariateSpline(phi1_knots, ln_phi1_rate, k=3)\n",
    "\n",
    "    # V = phi1_rate_spl.integral(*window)\n",
    "    _grid = jnp.linspace(*window, 1025)\n",
    "    lnV = ln_simpson(ln_phi1_rate_spl(_grid), _grid)\n",
    "\n",
    "    return -jnp.exp(lnV) / len(phi1_eval) + ln_phi1_rate_spl(phi1_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def ln_prob2(pars, data, phi2_knots):\n",
    "    n_phi2 = len(phi2_knots)\n",
    "    phi2_means = pars[:n_phi2]\n",
    "    ln_phi2_stds = pars[n_phi2 : 2 * n_phi2]\n",
    "    phi2_stds = jnp.exp(ln_phi2_stds)\n",
    "\n",
    "    phi1_rate = pars[2 * n_phi2 : 3 * n_phi2]\n",
    "\n",
    "    ll = phi2_ln_likelihood(\n",
    "        phi2_knots, phi2_means, phi2_knots, phi2_stds, data[\"phi1\"], data[\"phi2\"]\n",
    "    ).sum()\n",
    "\n",
    "    ll2 = phi1_ln_likelihood(\n",
    "        phi2_knots,\n",
    "        phi1_rate,\n",
    "        data[\"phi1\"],\n",
    "    ).sum()\n",
    "\n",
    "    lp = jln_normal(phi2_means, 0, 2.0).sum()\n",
    "    lp += jln_normal(ln_phi2_stds, -1, 1.0).sum()\n",
    "\n",
    "    return ll + ll2 + lp\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def objective2(pars, data, phi2_knots):\n",
    "    return -ln_prob2(pars, data, phi2_knots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, xe = np.histogram(data[\"phi1\"], bins=np.linspace(*window, 32))\n",
    "xc = 0.5 * (xe[:-1] + xe[1:])\n",
    "H = np.log((H + 1e-4) / (window[1] - window[0]))\n",
    "init_rate = InterpolatedUnivariateSpline(xc, H, k=3)(phi2_knots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.concatenate(\n",
    "    (phi2_vals, np.log(np.full_like(phi2_knots, 0.4)), init_rate)\n",
    ")\n",
    "data = {\"phi1\": phi1_data, \"phi2\": phi2_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ScipyMinimize(method=\"l-bfgs-b\", fun=objective2)\n",
    "res = solver.run(init_params, data=data, phi2_knots=phi2_knots)\n",
    "res.state.iter_num, res.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(phi2_knots, res.params[: len(phi2_knots)])\n",
    "plt.plot(phi2_knots, res.params[len(phi2_knots) : 2 * len(phi2_knots)], color=\"tab:red\")\n",
    "plt.plot(phi2_knots, res.params[2 * len(phi2_knots) :], color=\"tab:green\")\n",
    "plt.scatter(phi1_data, phi2_data, color=\"tab:blue\", s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(phi1_data.min(), phi1_data.max(), 256), np.linspace(-4, 4, 128)\n",
    ")\n",
    "zz = np.exp(\n",
    "    phi2_ln_likelihood(\n",
    "        phi2_knots,\n",
    "        res.params[: len(phi2_knots)],\n",
    "        phi2_knots,\n",
    "        np.exp(res.params[len(phi2_knots) : 2 * len(phi2_knots)]),\n",
    "        xx,\n",
    "        yy,\n",
    "    )\n",
    "    + phi1_ln_likelihood(\n",
    "        phi2_knots, res.params[2 * len(phi2_knots) : 3 * len(phi2_knots)], xx\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.pcolormesh(\n",
    "    xx,\n",
    "    yy,\n",
    "    zz,\n",
    "    shading=\"auto\",\n",
    "    vmin=np.median(zz[(xx > 0) & (xx < 5)]),\n",
    "    vmax=np.max(zz[(xx > 0) & (xx < 5)]),\n",
    ")\n",
    "# plt.scatter(phi1_data, phi2_data, color='tab:blue', s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd135d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = jax.tree_util.Partial(ln_prob2, data=data, phi2_knots=phi2_knots)\n",
    "warmup = blackjax.window_adaptation(\n",
    "    blackjax.nuts,\n",
    "    fn,\n",
    "    1000,\n",
    ")\n",
    "\n",
    "state, kernel, _ = warmup.run(\n",
    "    rng_key,\n",
    "    res.params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96caa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = inference_loop(rng_key, kernel, state, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "for i in np.random.choice(states.position.shape[0], size=32):\n",
    "    params = states.position[i]\n",
    "    plt.plot(phi2_knots, params[: len(phi2_knots)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "for i in np.random.choice(states.position.shape[0], size=32):\n",
    "    params = states.position[i]\n",
    "    plt.plot(phi2_knots, params[len(phi2_knots) : 2 * len(phi2_knots)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f88aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "for i in np.random.choice(states.position.shape[0], size=32):\n",
    "    params = states.position[i]\n",
    "    plt.plot(phi2_knots, params[2 * len(phi2_knots) : 3 * len(phi2_knots)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4eaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(phi1_data.min(), phi1_data.max(), 256), np.linspace(-4, 4, 128)\n",
    ")\n",
    "\n",
    "for i in np.random.choice(states.position.shape[0], size=8):\n",
    "    params = states.position[i]\n",
    "    zz = np.exp(\n",
    "        phi2_ln_likelihood(\n",
    "            phi2_knots,\n",
    "            params[: len(phi2_knots)],\n",
    "            phi2_knots,\n",
    "            np.exp(params[len(phi2_knots) : 2 * len(phi2_knots)]),\n",
    "            xx,\n",
    "            yy,\n",
    "        )\n",
    "        + phi1_ln_likelihood(\n",
    "            phi2_knots, params[2 * len(phi2_knots) : 3 * len(phi2_knots)], xx\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.pcolormesh(\n",
    "        xx,\n",
    "        yy,\n",
    "        zz,\n",
    "        shading=\"auto\",\n",
    "        vmin=np.median(zz[(xx > 0) & (xx < 5)]),\n",
    "        vmax=np.max(zz[(xx > 0) & (xx < 5)]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1545d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
